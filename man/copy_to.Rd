% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/copy_to_hdfs.R
\name{copy_to.RxHdfsFileSystem}
\alias{copy_to.RxHdfsFileSystem}
\alias{copy_to}
\alias{copy_to_hdfs}
\title{Upload a dataset to HDFS}
\usage{
\method{copy_to}{RxHdfsFileSystem}(dest, df, name = NULL, ...)

copy_to_hdfs(..., host = hdfs_host(), port = rxGetOption("hdfsPort"))
}
\arguments{
\item{dest}{The destination source: an object of class \code{\link{RxHdfsFileSystem}}.}

\item{df}{A dataset: can be a filename, an Xdf data source object, another RevoScaleR data source, or anything that can be coerced to a data frame.}

\item{name}{The filename, optionally including the path, for the uploaded Xdf file. The default upload location is the user's home directory (\code{user/<username>}) in the filesystem pointed to by \code{dest}.}

\item{...}{Further arguments; see below.}

\item{host, port}{HDFS hostname and port number to connect to. You should need to set these only if you have an attached Azure Data Lake Store that you are accessing via HDFS.}
}
\value{
An Xdf data source object pointing to the uploaded data.
}
\description{
Upload a dataset to HDFS
}
\details{
This is the RevoScaleR HDFS method for the dplyr \code{\link[dplyr]{copy_to}} function, for uploading data to a remote database/src. The method should work with any RevoScaleR data source, or with any R object that can be converted into a data frame. If the data is not already in Xdf format, it is first imported into Xdf, and then uploaded. Any arguments in \code{...} are passed to \code{\link{hdfs_download}}, and ultimately to the Hadoop \code{fs -copytoLocal} command.

The function will handle both the cases where you are logged into the edge node of a Hadoop/Spark cluster, and if you are a remote client. For the latter case, the uploading is a two-stage process: the data is first transferred to the native filesystem of the edge node, and then copied from the edge node into HDFS.

Similarly, it can handle uploading both to the host HDFS filesystem, and to an attached Azure Data Lake Store. If \code{dest} points to an ADLS host, the file will be uploaded to that filesystem, by default in the root directory. You can override this by supplying an explicit an explicit URI for the uploaded file, in the form \code{adl://azure.host.name/path}. The name for the host HDFS filesystem is \code{adl://host/}.

The \code{copy_to_hdfs} function is a simple wrapper that avoids having to create an explicit filesystem object. Its arguments other than \code{host} and \code{port} are simply passed as-is to \code{copy_to}.
}
\section{Note on composite Xdf}{

There are actually two kinds of Xdf files: standard and _composite_. A composite Xdf file is a directory containing multiple data and metadata files, which the RevoScaleR functions treat as a single dataset. Xdf files in HDFS must be composite in order to work properly; \code{copy_to} will convert an existing Xdf file into composite, if it's not already in that format. Non-Xdf datasets (data frames and other RevoScaleR data sources, such as text files) will similarly be uploaded as composite.
}

\examples{
\dontrun{
hd <- RxHdfsFileSystem()

# copy a data frame to HDFS
mth <- copy_to(hd, mtcars)
# assign a new filename on copy
mth2 <- copy_to(hd, mtcars, "mtcars_2")

# copy an Xdf file to HDFS
mtx <- as_xdf(mtcars)
mth3 <- copy_to(hd, mtx, "mtcars_3")

# same as copy_to(hd, ...)
delete_xdf(mth)
copy_to_hdfs(mtcars)

# copying to attached ADLS storage
copy_to_hdfs(mtcars, host="adls.host.name")
}
}
\seealso{
\code{\link{rxHadoopCopyFromClient}}, \code{\link{rxHadoopCopyFromLocal}},
\code{\link{collect}} and \code{\link{compute}} for downloading data from HDFS,
\code{\link{as_xdf}}, \code{\link{as_composite_xdf}}
}
