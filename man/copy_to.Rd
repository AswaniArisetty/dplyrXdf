% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/copy_to_hdfs.R
\name{copy_to.RxHdfsFileSystem}
\alias{copy_to.RxHdfsFileSystem}
\alias{copy_to}
\alias{copy_to_hdfs}
\title{Upload a dataset to HDFS}
\usage{
\method{copy_to}{RxHdfsFileSystem}(dest, df, path = NULL, overwrite = FALSE,
  force_composite = TRUE, ...)

copy_to_hdfs(...)
}
\arguments{
\item{dest}{The destination source: an object of class \code{\link{RxHdfsFileSystem}}.}

\item{df}{A dataset: can be a filename, an Xdf data source object, another RevoScaleR data source, or anything that can be coerced to a data frame.}

\item{path}{The HDFS directory in which to store the uploaded dataset. Defaults to the user's HDFS home directory.}

\item{overwrite}{Whether to overwrite any existing file.}

\item{...}{For \code{copy_to}, further arguments to \code{\link{rxHadoopCommand}}.}

\item{force_composite:}{Whether to force the uploaded dataset to be a composite Xdf file. See details below.}
}
\value{
An Xdf data source object pointing to the uploaded data.
}
\description{
Upload a dataset to HDFS
}
\details{
This is the RevoScaleR HDFS method for the dplyr \code{\link[dplyr]{copy_to}} function, for uploading data to a remote database/src. The method should work with any RevoScaleR data source, or with any R object that can be converted into a data frame. If the data is not already in Xdf format, it is first imported into Xdf, and then uploaded.

The code will handle both the cases where you are logged into the edge node of a Hadoop/Spark cluster, and if you are a remote client. For the latter case, the uploading is a two-stage process: the data is first transferred to the native filesystem of the edge node, and then copied from the edge node into HDFS.

The \code{copy_to_hdfs} function is a simple wrapper that avoids having to create an explicit filesystem object.
}
\seealso{
\code{\link{rxHadoopCopyFromClient}}, \code{\link{rxHadoopCopyFromLocal}},
\code{\link{collect}} and \code{\link{compute}} for downloading data from HDFS
}
