% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/copy_to.R
\name{copy_to.RxDataSource}
\alias{copy_to.RxDataSource}
\alias{copy_to}
\alias{copy_to.RxHdfsFileSystem}
\alias{copy_to_hdfs}
\title{Upload a dataset to a remote backend}
\usage{
\method{copy_to}{RxDataSource}(dest, df, ...)

\method{copy_to}{RxHdfsFileSystem}(dest, df, name = NULL, ...)

copy_to_hdfs(..., host = hdfs_host(), port = rxGetOption("hdfsPort"))
}
\arguments{
\item{dest}{The destination source: either a RevoScaleR data source object, or a filesystem object of class \code{\link{RxHdfsFileSystem}}.}

\item{df}{A dataset. For the \code{RxDataSource} method, this can be any RevoScaleR data source object, presumably of a different class to the destination. For the \code{RxHdfsFileSystem} method, this can be the filename of an Xdf file, a RevoScaleR data source, or anything that can be coerced to a data frame.}

\item{...}{Further arguments to lower-level functions; see below.}

\item{name}{The filename, optionally including the path, for the uploaded Xdf file. The default upload location is the user's home directory (\code{user/<username>}) in the filesystem pointed to by \code{dest}. Not used for the \code{RxDataSource} method.}

\item{host, port}{HDFS hostname and port number to connect to. You should need to set these only if you have an attached Azure Data Lake Store that you are accessing via HDFS.}
}
\value{
An Xdf data source object pointing to the uploaded data.
}
\description{
Upload a dataset to a remote backend
}
\details{
RevoScaleR does not have an exact analogue of the dplyr concept of a src, and because of this, the dplyrXdf implementation of \code{copy_to} is somewhat different. In dplyrXdf, the function serves two related, overlapping purposes:
\itemize{
   \item First, it can be used to copy a dataset to a different \emph{format}, for example from an Xdf file to a SQL Server database. To do this, \code{dest} should be a data source object of the target class (\code{RxSqlServerData} for SQL Server), specifying the name/location of the copied data.
   \item Second, it can be used to upload a dataset to a different \code{filesystem}, such as the HDFS filesystem of a Hadoop or Spark cluster. The dataset will be saved in Xdf format. For this, \code{dest} should be a \code{RxHdfsFileSystem} object.
}

The \code{copy_to_hdfs} function is a simple wrapper to the HDFS upload method that avoids having to create an explicit filesystem object. Its arguments other than \code{host} and \code{port} are simply passed as-is to \code{copy_to.RxHdfsFileSystem}.

The method for uploading to HDFS can handle both the cases where you are logged into the edge node of a Hadoop/Spark cluster, and where you are a remote client. For the latter case, the uploading is a two-stage process: the data is first transferred to the native filesystem of the edge node, and then copied from the edge node into HDFS. Similarly, it can handle uploading both to the host HDFS filesystem, and to an attached Azure Data Lake Store. If \code{dest} points to an ADLS host, the file will be uploaded there. You can override this by supplying an explicit an explicit URI for the uploaded file, in the form \code{adl://azure.host.name/path}. The name for the host HDFS filesystem is \code{adl://host/}.

For the HDFS upload method, any arguments in \code{...} are passed to \code{\link{hdfs_upload}}, and ultimately to the Hadoop \code{fs -copytoLocal} command. For the data source copy method, arguments in \code{...} are passed to \code{\link{rxDataStep}}.

\code{copy_to} is meant for copying \emph{datasets} to different backends. If you are simply copying a file to HDFS, consider using \code{hdfs_upload}; or if you are copying an Xdf file to a different location in the same filesystem, use \code{copy_xdf} or \code{\link{file.copy}}.
}
\section{Note on composite Xdf}{

There are actually two kinds of Xdf files: standard and \emph{composite}. A composite Xdf file is a directory containing multiple data and metadata files, which the RevoScaleR functions treat as a single dataset. Xdf files in HDFS must be composite in order to work properly; \code{copy_to} will convert an existing Xdf file into composite, if it's not already in that format. Non-Xdf datasets (data frames and other RevoScaleR data sources, such as text files) will similarly be uploaded as composite.
}

\examples{
\dontrun{
# copy a data frame to SQL Server
connStr <- "SERVER=hostname;DATABASE=RevoTestDB;TRUSTED_CONNECTION=yes"
mtdb <- RxSqlServerData("mtcars", connectionString=connString)
copy_to(mtdb, mtcars)

# copy an Xdf file to SQL Server: will overwrite any existing table with the same name
mtx <- as_xdf(mtcars)
copy_to(mtdb, mtx)

# copy a data frame to HDFS
hd <- RxHdfsFileSystem()
mth <- copy_to(hd, mtcars)
# assign a new filename on copy
mth2 <- copy_to(hd, mtcars, "mtcars_2")

# copy an Xdf file to HDFS
mth3 <- copy_to(hd, mtx, "mtcars_3")

# same as copy_to(hd, ...)
delete_xdf(mth)
copy_to_hdfs(mtcars)

# copying to attached ADLS storage
copy_to_hdfs(mtcars, host="adl://adls.host.name")
}
}
\seealso{
\code{\link{rxHadoopCopyFromClient}}, \code{\link{rxHadoopCopyFromLocal}},
\code{\link{collect}} and \code{\link{compute}} for downloading data from HDFS,
\code{\link{as_xdf}}, \code{\link{as_composite_xdf}}
}
