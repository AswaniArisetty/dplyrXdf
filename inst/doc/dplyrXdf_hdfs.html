<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Hong Ooi" />

<meta name="date" content="2017-09-18" />

<title>Working with HDFS</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Working with HDFS</h1>
<h4 class="author"><em>Hong Ooi</em></h4>
<h4 class="date"><em>2017-09-18</em></h4>


<div id="TOC">
<ul>
<li><a href="#basics-hadoop-spark-and-hdfs">Basics: Hadoop, Spark and HDFS</a></li>
<li><a href="#working-with-a-cluster">Working with a cluster</a><ul>
<li><a href="#uploading-and-downloading">Uploading and downloading</a></li>
<li><a href="#note-on-composite-datasets">Note on composite datasets</a></li>
</ul></li>
<li><a href="#sparklyr-interoperability-and-hive-tables">Sparklyr interoperability and Hive tables</a></li>
<li><a href="#working-with-files-and-directories">Working with files and directories</a><ul>
<li><a href="#azure-data-lake-store-support">Azure Data Lake Store support</a></li>
</ul></li>
<li><a href="#miscellaneous-functions">Miscellaneous functions</a></li>
<li><a href="#limitations">Limitations</a></li>
</ul>
</div>

<!-- to build this vignette as part of installation:
 - copy it to the /vignettes directory
 - ensure you are connected to a Spark or Hadoop cluster (a remote connection works)
 - run devtools::install(build_vignettes=TRUE) -->
<p>Microsoft R Server includes the ability to work with Hadoop and Spark clusters. As part of this, it also lets you work with datasets, in particular Xdf files, that are stored in HDFS.</p>
<p>As of version 0.10.0, dplyrXdf also supports Xdf files in HDFS. Most verbs will work exactly the same as with data in the native filesystem, and there are also a number of utility functions to ease working with Hadoop and Spark. You will need to have Microsoft R Server version 9.0 to use dplyrXdf with HDFS, and ideally version 9.1 for full functionality.</p>
<div id="basics-hadoop-spark-and-hdfs" class="section level2">
<h2>Basics: Hadoop, Spark and HDFS</h2>
<p>If you’re used to dplyr and how it handles databases, Microsoft R Server and dplyrXdf work a little differently. What dplyr calls a “src” (a remote data source) is handled by two related concepts in MRS: a <em>compute context</em> and a <em>filesystem</em>. The compute context determines where computations take place: on a local machine, in a database like SQL Server, or in a Hadoop or Spark cluster. The <em>filesystem</em> is where the data files are stored. Note that not all data sources have a filesystem: eg a SQL Server table is part of a database, not a filesystem as such.</p>
<p>In general, there are only two filesystems you should have to deal with: the native filesystem, meaning the hard disk of the machine where your R session is running, and HDFS. To MRS and dplyrXdf, an Xdf file is much the same regardless of which filesystem it’s in (there are some limitations for HDFS files, but they are the exception, not the rule). You can use the same code and pipelines without having to worry about filesystem details.</p>
<p>The compute context is important because it’s how you benefit from the parallelism provided by Hadoop and Spark. Note that it doesn’t change the way you <em>write</em> your code – only how it’s executed.</p>
<ul>
<li><p>If you are in the <code>RxHadoopMR</code> or <code>RxSpark</code> compute context, <em>and</em> your data is in HDFS, then the RevoScaleR functions will execute in parallel on the cluster worker nodes. You can do this in an R session either on a remote client, or on the cluster edge node.</p></li>
<li><p>If you are in the local compute context, and your R session is running on the cluster edge node, you can still access data in HDFS. In this case, the data is streamed to the edge node for processing – essentially, RevoScaleR will treat HDFS as simply a big local hard disk. (This will not work if you are on a remote client; in this case, you must be in the Hadoop or Spark compute context to access the cluster.)</p></li>
<li><p>If you are in the <code>RxHadoopMR</code> or <code>RxSpark</code> compute context and your data is <em>not</em> in HDFS, then the RevoScaleR functions will (usually) throw a warning or error.</p></li>
</ul>
<p>For more information about working with HDFS, Hadoop and Spark, see the <a href="https://docs.microsoft.com/en-us/r-server/r/how-to-revoscaler-spark">documentation at Microsoft Docs</a>, or the Microsoft Learn Analytics <a href="https://github.com/Azure/LearnAnalytics-mrs-spark">course materials on Spark</a>.</p>
</div>
<div id="working-with-a-cluster" class="section level2">
<h2>Working with a cluster</h2>
<p>To use dplyrXdf on a cluster, you’ll have to install it, and dplyr, on the machine where you’ll be running your R sessions. This can be either the cluster edge node, or if you’re connecting remotely, on your remote machine. For full functionality, you should also install dplyr on the cluster worker nodes. You don’t need to install dplyrXdf on the worker nodes, though.</p>
<p>Your dplyrXdf code should work the same regardless of whether you’re running on the edge node or a remote client. The package abstracts away the underlying differences between the two scenarios, so that from your point of view, you’re simply connected to a cluster somewhere in the cloud. Similarly, dplyrXdf should work with all flavours of Spark and Hadoop that MRS supports (HDInsight, Cloudera, Hortonworks and MapR).</p>
<div id="uploading-and-downloading" class="section level3">
<h3>Uploading and downloading</h3>
<p>There are a number of ways to get your data into and out of HDFS. Note that there are several data formats in use in Hadoop and Spark environments; dplyrXdf focuses mainly on Xdf files.</p>
<p>To copy a dataset from the native filesystem into HDFS, use <code>copy_to</code>. This is the standard dplyr verb for copying data into a remote src. In this case, the src is a filesystem, which RevoScaleR represents via a <code>RxHdfsFileSystem</code> object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyrXdf)
<span class="kw">library</span>(nycflights13)

hd &lt;-<span class="st"> </span><span class="kw">RxHdfsFileSystem</span>()

<span class="co"># copy a data frame into an Xdf file in HDFS</span>
flightsHd &lt;-<span class="st"> </span><span class="kw">copy_to</span>(hd, flights)
<span class="co">#&gt; Creating composite Xdf from non-composite data source</span>
<span class="co">#&gt; 17/09/18 10:06:41 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is &lt;= 1</span>
<span class="co">#&gt; 17/09/18 10:06:42 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 145 ms with threads: 0</span>

flightsHd
<span class="co">#&gt; RxXdfData Source</span>
<span class="co">#&gt; &quot;/user/sshuser/flights&quot;</span>
<span class="co">#&gt; fileSystem: </span>
<span class="co">#&gt;     fileSystemType: hdfs</span>
<span class="co">#&gt;     hostName: default</span>
<span class="co">#&gt;     port: 0</span>
<span class="co">#&gt;     useWebHdfs: FALSE</span>
<span class="co">#&gt;     verbose: FALSE</span>
<span class="co">#&gt; createCompositeSet: TRUE</span>

<span class="kw">as_data_frame</span>(flightsHd)
<span class="co">#&gt; # A tibble: 336,776 x 19</span>
<span class="co">#&gt;    year month   day dep_t. sche. dep_. arr_. sche. arr_. carr. flig. tail.</span>
<span class="co">#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;</span>
<span class="co">#&gt; 1  2013     1     1    517   515  2.00   830   819  11.0 UA     1545 N142.</span>
<span class="co">#&gt; 2  2013     1     1    533   529  4.00   850   830  20.0 UA     1714 N242.</span>
<span class="co">#&gt; 3  2013     1     1    542   540  2.00   923   850  33.0 AA     1141 N619.</span>
<span class="co">#&gt; 4  2013     1     1    544   545 -1.00  1004  1022 -18.0 B6      725 N804.</span>
<span class="co">#&gt; 5  2013     1     1    554   600 -6.00   812   837 -25.0 DL      461 N668.</span>
<span class="co">#&gt; # ... with 336,771 more rows, and 7 more variables: origin &lt;chr&gt;,</span>
<span class="co">#&gt; #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,</span>
<span class="co">#&gt; #   time_hour &lt;dttm&gt;</span></code></pre></div>
<p>This will copy the flights table into HDFS, returning an Xdf data source object pointing to the file. If the path isn’t specified, the data is saved as an Xdf file in your HDFS home directory, normally <code>/user/&lt;username&gt;</code>.</p>
<p>You can also use <code>copy_to_hdfs</code>, which is a shortcut that saves having to create an explicit filesystem object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># same as above</span>
flightsHd &lt;-<span class="st"> </span><span class="kw">copy_to_hdfs</span>(flights)</code></pre></div>
<p>To download an Xdf file from HDFS to the native filesystem, use <code>collect</code> and <code>compute</code>. Again, these are the standard dplyr verbs for copying data from a remote src. For dplyrXdf, the difference between <code>collect</code> and <code>compute</code> is that the former by default will return a data frame, whereas the latter will save the copied data in an Xdf file and return a <code>tbl_xdf</code> object pointing to that file.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flightsLocal &lt;-<span class="st"> </span><span class="kw">compute</span>(flightsHd)

flightsLocal
<span class="co">#&gt; tbl_xdf Source</span>
<span class="co">#&gt; &quot;d:\misc\Rtemp\RtmpEduo1t\dxTmp34a851e20e4/flights&quot;</span>
<span class="co">#&gt; fileSystem: </span>
<span class="co">#&gt;     fileSystemType: native</span>
<span class="co">#&gt; createCompositeSet: TRUE</span>

<span class="kw">as_data_frame</span>(flightsLocal)
<span class="co">#&gt; # A tibble: 336,776 x 19</span>
<span class="co">#&gt;    year month   day dep_t. sche. dep_. arr_. sche. arr_. carr. flig. tail.</span>
<span class="co">#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;</span>
<span class="co">#&gt; 1  2013     1     1    517   515  2.00   830   819  11.0 UA     1545 N142.</span>
<span class="co">#&gt; 2  2013     1     1    533   529  4.00   850   830  20.0 UA     1714 N242.</span>
<span class="co">#&gt; 3  2013     1     1    542   540  2.00   923   850  33.0 AA     1141 N619.</span>
<span class="co">#&gt; 4  2013     1     1    544   545 -1.00  1004  1022 -18.0 B6      725 N804.</span>
<span class="co">#&gt; 5  2013     1     1    554   600 -6.00   812   837 -25.0 DL      461 N668.</span>
<span class="co">#&gt; # ... with 336,771 more rows, and 7 more variables: origin &lt;chr&gt;,</span>
<span class="co">#&gt; #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,</span>
<span class="co">#&gt; #   time_hour &lt;dttm&gt;</span></code></pre></div>
<p>If you want to look at the first few rows of an Xdf file, it may be faster to use <code>compute</code> to copy the entire file off HDFS, and then run <code>head</code>, than to run <code>head</code> on the original. This is due to quirks in how RevoScaleR works in Spark and Hadoop.</p>
<p><code>copy_to</code>, <code>collect</code> and <code>compute</code> are for copying <em>datasets</em> (R objects, including data frames and Xdf files) to and from HDFS. To transfer arbitrary files and directories, dplyrXdf also provides the <code>hdfs_upload</code> and <code>hdfs_download</code> functions. If you’ve used the base R function <code>download.file</code> or a command-line file transfer utility like <code>ftp</code> or <code>pscp</code>, the syntax should be familiar: <code>hdfs_download(src, dest)</code> downloads the file at <code>src</code> to the location <code>dest</code>, while <code>hdfs_upload(src, dest)</code> uploads <code>src</code> to <code>dest</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a csv file and upload it</span>
<span class="kw">write.csv</span>(flights, <span class="st">&quot;flights.csv&quot;</span>, <span class="dt">row.names=</span><span class="ot">FALSE</span>)
<span class="kw">hdfs_upload</span>(<span class="st">&quot;flights.csv&quot;</span>, <span class="st">&quot;/tmp&quot;</span>)
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
<p>If you uploaded a non-Xdf data source (like a csv file), you can then import it into Xdf format with <code>as_xdf</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flightsCsv &lt;-<span class="st"> </span><span class="kw">RxTextData</span>(<span class="st">&quot;/tmp/flights.csv&quot;</span>, <span class="dt">fileSystem=</span><span class="kw">RxHdfsFileSystem</span>())
flightsHd2 &lt;-<span class="st"> </span><span class="kw">as_xdf</span>(flightsCsv, <span class="st">&quot;flights2&quot;</span>)

<span class="kw">as_data_frame</span>(flightsHd2)
<span class="co">#&gt; # A tibble: 336,776 x 19</span>
<span class="co">#&gt;    year month   day dep_t. sche. dep_. arr_. sche. arr_. carr. flig. tail.</span>
<span class="co">#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;</span>
<span class="co">#&gt; 1  2013     1     1    517   515     2   830   819    11 UA     1545 N142.</span>
<span class="co">#&gt; 2  2013     1     1    533   529     4   850   830    20 UA     1714 N242.</span>
<span class="co">#&gt; 3  2013     1     1    542   540     2   923   850    33 AA     1141 N619.</span>
<span class="co">#&gt; 4  2013     1     1    544   545    -1  1004  1022   -18 B6      725 N804.</span>
<span class="co">#&gt; 5  2013     1     1    554   600    -6   812   837   -25 DL      461 N668.</span>
<span class="co">#&gt; # ... with 336,771 more rows, and 7 more variables: origin &lt;chr&gt;,</span>
<span class="co">#&gt; #   dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;,</span>
<span class="co">#&gt; #   time_hour &lt;chr&gt;</span></code></pre></div>
</div>
<div id="note-on-composite-datasets" class="section level3">
<h3>Note on composite datasets</h3>
<p>Note that there are two kinds of Xdf files: standard and <em>composite</em>. A composite Xdf file is actually a directory containing multiple data and metadata files. The RevoScaleR functions can treat a composite Xdf as a single dataset, and so can dplyrXdf. Xdf files in HDFS must be composite in order to work properly; by default, <code>copy_to</code> will convert an existing Xdf file into composite, if it’s not already in that format.</p>
</div>
</div>
<div id="sparklyr-interoperability-and-hive-tables" class="section level2">
<h2>Sparklyr interoperability and Hive tables</h2>
<p>If your R session is on the edge node, RevoScaleR provides the ability to share a Spark connection with the <a href="https://spark.rstudio.com">sparklyr package</a> from RStudio. You do this via the <code>interop</code> argument to <code>rxSparkConnect</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># establishes a Spark connection that can be shared with sparklyr</span>
<span class="kw">rxSparkConnect</span>(..., <span class="dt">interop=</span><span class="st">&quot;sparklyr&quot;</span>)</code></pre></div>
<p>dplyrXdf can take advantage of this interoperability when working with Hive tables. To see how this works, let’s use the <code>hivesampledata</code> table which is provided as part of every Azure HDInsight cluster:</p>
<!-- to render this programmatically, you must be on the edge node -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cc &lt;-<span class="st"> </span><span class="kw">rxSparkConnect</span>(<span class="dt">interop=</span><span class="st">&quot;sparklyr&quot;</span>)

sampleHiv &lt;-<span class="st"> </span><span class="kw">RxHiveData</span>(<span class="dt">table=</span><span class="st">&quot;hivesampletable&quot;</span>)
<span class="kw">head</span>(sampleHiv, <span class="dv">5</span>)
<span class="co">#&gt;   clientid querytime market deviceplatform devicemake devicemodel        state</span>
<span class="co">#&gt; 1        8  18:54:20  en-US        Android    Samsung    SCH-i500   California</span>
<span class="co">#&gt; 2       23  19:19:44  en-US        Android        HTC  Incredible Pennsylvania</span>
<span class="co">#&gt; 3       23  19:19:46  en-US        Android        HTC  Incredible Pennsylvania</span>
<span class="co">#&gt; 4       23  19:19:47  en-US        Android        HTC  Incredible Pennsylvania</span>
<span class="co">#&gt; 5       28  01:37:50  en-US        Android   Motorola     Droid X     Colorado</span>
<span class="co">#&gt;         country querydwelltime sessionid sessionpagevieworder</span>
<span class="co">#&gt; 1 United States      13.920401         0                    0</span>
<span class="co">#&gt; 2 United States             NA         0                    0</span>
<span class="co">#&gt; 3 United States       1.475742         0                    1</span>
<span class="co">#&gt; 4 United States       0.245968         0                    2</span>
<span class="co">#&gt; 5 United States      20.309534         1                    1</span>

sampleHiv %&gt;%
<span class="st">    </span><span class="kw">filter</span>(deviceplatform ==<span class="st"> &quot;Android&quot;</span>) %&gt;%
<span class="st">    </span><span class="kw">group_by</span>(devicemake) %&gt;%
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">n=</span><span class="kw">n</span>()) %&gt;%
<span class="st">    </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n)) %&gt;%
<span class="st">    </span><span class="kw">head</span>()
<span class="co">#&gt; # Source:     lazy query [?? x 2]</span>
<span class="co">#&gt; # Database:   spark_connection</span>
<span class="co">#&gt; # Ordered by: desc(n)</span>
<span class="co">#&gt;     devicemake     n</span>
<span class="co">#&gt;          &lt;chr&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1      Samsung 16244</span>
<span class="co">#&gt; 2           LG  7950</span>
<span class="co">#&gt; 3          HTC  2242</span>
<span class="co">#&gt; 4      Unknown  2133</span>
<span class="co">#&gt; 5     Motorola  1524</span>
<span class="co">#&gt; # ... with more rows</span></code></pre></div>
<p>In this example, dplyrXdf has converted the <code>RxHiveData</code> source into a sparklyr tbl, and the subsequent pipeline will be run by sparklyr (<em>not</em> by dplyrXdf) in the cluster.</p>
<p>If you don’t want to use sparklyr – for example, if you want to take advantage of MRS features like a <code>transformFunc</code> to execute arbitrary R code – you can import the Hive table to an Xdf file in HDFS. The pipeline will then execute according to your compute context. This will probably still be in-cluster, since you must be in the Spark compute context to access Hive tables. However, executing the pipeline via sparklyr is likely to be more efficient, since it removes the need to import the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># this will create the composite Xdf 'samplehivetable' in the HDFS user directory</span>
sampleXdf &lt;-<span class="st"> </span><span class="kw">as_xdf</span>(sampleHiv)

sampleXdf %&gt;%
<span class="st">    </span><span class="kw">filter</span>(deviceplatform ==<span class="st"> &quot;Android&quot;</span>) %&gt;%
<span class="st">    </span><span class="kw">group_by</span>(devicemake) %&gt;%
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">n=</span><span class="kw">n</span>()) %&gt;%
<span class="st">    </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n)) %&gt;%
<span class="st">    </span><span class="kw">head</span>()
<span class="co">#&gt;     devicemake     n</span>
<span class="co">#&gt; 1      Samsung 16244</span>
<span class="co">#&gt; 2           LG  7950</span>
<span class="co">#&gt; 3          HTC  2242</span>
<span class="co">#&gt; 4      Unknown  2133</span>
<span class="co">#&gt; 5     Motorola  1524</span></code></pre></div>
</div>
<div id="working-with-files-and-directories" class="section level2">
<h2>Working with files and directories</h2>
<p>dplyrXdf provides the following functions to let you manipulate files and directories in HDFS. By and large they wrap similar functions provided by the RevoScaleR package, which in turn call various Hadoop filesystem commands.</p>
<ul>
<li><code>hdfs_file_copy(src, dest)</code> copies the file or directory given by <code>src</code> to the location <code>dest</code>. It is vectorised in both <code>src</code> and <code>dest</code>, meaning <code>src1</code> will be copied to <code>dest1</code>, <code>src2</code> to <code>dest2</code>, and so on. It is analogous to base R’s <code>file.copy</code> for the native filesystem.</li>
<li><code>hdfs_file_move(src, dest)</code> is similar, but moves files/directories. It is analogous to base R’s <code>file.rename</code>.</li>
<li><code>hdfs_file_remove</code> deletes the path. Its counterpart in base R is <code>file.remove</code>.</li>
<li><code>hdfs_dir_create</code> and <code>hdfs_dir_remove</code> create and delete directories. They are analogous to <code>dir.create</code> and <code>unlink(recursive=TRUE)</code>.</li>
<li><code>hdfs_file_exists</code> and <code>hdfs_dir_exists</code> test for the existence of a file or directory, like base R’s <code>file.exists</code> and <code>dir.exists</code>.</li>
<li><code>hdfs_dir</code> lists files in a HDFS directory, returning a vector of file names. It has a number of options for recursively listing subdirectories, returning subdirectories only (omitting files), etc.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a new directory</span>
<span class="kw">hdfs_dir_create</span>(<span class="st">&quot;/tmp/mydata&quot;</span>)
<span class="co">#&gt; [1] TRUE</span>

<span class="co"># check that it exists</span>
<span class="kw">hdfs_dir_exists</span>(<span class="st">&quot;/tmp/mydata&quot;</span>)
<span class="co">#&gt; [1] TRUE</span>

<span class="co"># copy files into the new directory</span>
<span class="kw">hdfs_file_copy</span>(<span class="st">&quot;flights&quot;</span>, <span class="st">&quot;/tmp/mydata&quot;</span>)
<span class="co">#&gt; [1] TRUE</span>

<span class="co"># create a new data source</span>
flightsHd3 &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>(<span class="st">&quot;/tmp/mydata/flights&quot;</span>, <span class="dt">fileSystem=</span><span class="kw">RxHdfsFileSystem</span>())

<span class="co"># read the data</span>
<span class="kw">names</span>(flightsHd3)
<span class="co">#&gt;  [1] &quot;year&quot;           &quot;month&quot;          &quot;day&quot;            &quot;dep_time&quot;      </span>
<span class="co">#&gt;  [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot;      &quot;arr_time&quot;       &quot;sched_arr_time&quot;</span>
<span class="co">#&gt;  [9] &quot;arr_delay&quot;      &quot;carrier&quot;        &quot;flight&quot;         &quot;tailnum&quot;       </span>
<span class="co">#&gt; [13] &quot;origin&quot;         &quot;dest&quot;           &quot;air_time&quot;       &quot;distance&quot;      </span>
<span class="co">#&gt; [17] &quot;hour&quot;           &quot;minute&quot;         &quot;time_hour&quot;</span></code></pre></div>
<div id="azure-data-lake-store-support" class="section level3">
<h3>Azure Data Lake Store support</h3>
<p>As with RevoScaleR, dplyrXdf supports HDFS data located in attached ADLS storage. When working with Xdf files, you will normally specify the HDFS filesystem as part of the <code>fileSystem</code> argument. The standard dplyr/dplyrXdf verbs can both read and write to ADLS, with the location of the file being essentially transparent to the user.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">adlFs &lt;-<span class="st"> </span><span class="kw">RxHdfsFileSystem</span>(<span class="dt">hostName=</span><span class="st">&quot;adl://mycluster.azuredatalakestore.net&quot;</span>)
adlXdf &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>(<span class="st">&quot;/path/to/file&quot;</span>, <span class="dt">fileSystem=</span>adlFs)

<span class="co"># adlXdf %&gt;% verb1 %&gt;% verb2 %&gt;% ...</span></code></pre></div>
<p>The <code>hdfs_*</code> functions include a <code>host</code> argument to let you specify which HDFS filesystem to access: either the “native” filesystem, or ADLS. The default will be whichever filesystem is currently being used. For example the following code uses <code>hdfs_dir</code> to list the files in the root directory, first for the native HDFS filesystem, and then for an attached ADLS filesystem.</p>
<!-- to render this programmatically, your cluster must have ADLS storage attached -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdfs_dir</span>(<span class="st">&quot;/&quot;</span>, <span class="dt">host=</span><span class="st">&quot;default&quot;</span>)
<span class="co">#&gt; Directory listing of /</span>
<span class="co">#&gt;  [1] &quot;HdiNotebooks&quot;             &quot;HdiSamples&quot;</span>
<span class="co">#&gt;  [3] &quot;ams&quot;                      &quot;amshbase&quot;</span>
<span class="co">#&gt;  [5] &quot;app-logs&quot;                 &quot;apps&quot;</span>
<span class="co">#&gt;  [7] &quot;atshistory&quot;               &quot;cluster-info&quot;</span>
<span class="co">#&gt;  [9] &quot;custom-scriptaction-logs&quot; &quot;example&quot;</span>
<span class="co">#&gt; [11] &quot;hbase&quot;                    &quot;hdp&quot;</span>
<span class="co">#&gt; [13] &quot;hive&quot;                     &quot;mapred&quot;</span>
<span class="co">#&gt; [15] &quot;mr-history&quot;               &quot;tmp&quot;</span>
<span class="co">#&gt; [17] &quot;user&quot;</span>

<span class="kw">hdfs_dir</span>(<span class="st">&quot;/&quot;</span>, <span class="dt">host=</span><span class="st">&quot;adl://mycluster.azuredatalakestore.net&quot;</span>)
<span class="co">#&gt; Directory listing of adl://mycluster.azuredatalakestore.net/</span>
<span class="co">#&gt; [1] &quot;clusters&quot;         &quot;file31bc3ee52d32&quot; &quot;file31bc79a3de7&quot;  &quot;file5d301ef514e5&quot;</span>
<span class="co">#&gt; [5] &quot;folder1&quot;          &quot;tmp&quot;              &quot;user&quot;</span></code></pre></div>
<p>You can also provide a full URI as a path:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdfs_dir</span>(<span class="st">&quot;adl://mycluster.azuredatalakestore.net/&quot;</span>)
<span class="co">#&gt; Directory listing of adl://mycluster.azuredatalakestore.net/</span>
<span class="co">#&gt; [1] &quot;clusters&quot;         &quot;file31bc3ee52d32&quot; &quot;file31bc79a3de7&quot;  &quot;file5d301ef514e5&quot;</span>
<span class="co">#&gt; [5] &quot;folder1&quot;          &quot;tmp&quot;              &quot;user&quot;</span></code></pre></div>
<p>The <code>hdfs_host</code> function returns the name of the currently active HDFS filesystem host, or if an Xdf data source is supplied, the host for that data source:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdfs_host</span>()
<span class="co">#&gt; [1] &quot;adl://mycluster.azuredatalakestore.net&quot;</span>

xdf &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>(<span class="st">&quot;file&quot;</span>, <span class="dt">fileSystem=</span><span class="kw">RxHdfsFileSystem</span>(<span class="dt">hostName=</span><span class="st">&quot;default&quot;</span>))
<span class="kw">hdfs_host</span>(xdf)
<span class="co">#&gt; [1] &quot;default&quot;</span></code></pre></div>
</div>
</div>
<div id="miscellaneous-functions" class="section level2">
<h2>Miscellaneous functions</h2>
<p>The <code>in_hdfs</code> function returns whether a given data source is stored in HDFS.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">in_hdfs</span>(flightsHd)
<span class="co">#&gt; [1] TRUE</span>

<span class="co"># also works with non-Revo data sources, like data frames</span>
<span class="kw">in_hdfs</span>(iris)
<span class="co">#&gt; [1] FALSE</span>

<span class="kw">in_hdfs</span>(flights)
<span class="co">#&gt; [1] FALSE</span></code></pre></div>
<p>The <code>local_exec</code> function runs an expression in the local compute context. This can be useful if you need to work with local datasets while connected to a remote cluster. By default, RevoScaleR functions will throw an error if you provide a local data source as an input when you are in the Hadoop or Spark compute context. <code>local_exec</code> temporarily changes to the local compute context, runs your code, and then changes back to the original context.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># try to access a local Xdf file</span>
<span class="kw">names</span>(flightsLocal)
<span class="co">#&gt; Error: RxXdfData as input data source in RxSpark compute context must be in hdfs file system</span>

<span class="kw">local_exec</span>(<span class="kw">names</span>(flightsLocal))
<span class="co">#&gt;  [1] &quot;year&quot;           &quot;month&quot;          &quot;day&quot;            &quot;dep_time&quot;      </span>
<span class="co">#&gt;  [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot;      &quot;arr_time&quot;       &quot;sched_arr_time&quot;</span>
<span class="co">#&gt;  [9] &quot;arr_delay&quot;      &quot;carrier&quot;        &quot;flight&quot;         &quot;tailnum&quot;       </span>
<span class="co">#&gt; [13] &quot;origin&quot;         &quot;dest&quot;           &quot;air_time&quot;       &quot;distance&quot;      </span>
<span class="co">#&gt; [17] &quot;hour&quot;           &quot;minute&quot;         &quot;time_hour&quot;</span></code></pre></div>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<p>Most of the single-table dplyr verbs supported by dplyrXdf will work for datasets in HDFS. The main exceptions are:</p>
<ul>
<li><code>distinct</code>, <code>sample_n</code> and <code>sample_frac</code> do not support HDFS data</li>
<li><code>arrange</code>,<code>cbind</code> and <code>rbind</code> require the compute context to be local</li>
<li>grouped <code>mutate</code> and <code>transmute</code> also require that the compute context is local. Consider whether you really need to group before transforming; many transformations do not require grouping information. If your data fits into memory, you can also use <code>do</code> or <code>do_xdf</code>.</li>
<li><code>summarise</code> on HDFS data will always send the output to the edge node/remote client, before writing it back to the cluster. This is a consequence of the way in which <code>rxCube</code> and <code>rxSummary</code> work, by creating an in-memory data structure.</li>
</ul>
<p>Support for two-table verbs is similarly more limited for datasets in HDFS than in the native filesystem. First, only the Spark compute context supports joining (not Hadoop), and only for Xdf data sources and Spark data sources (<code>RxHiveData</code>, <code>RxOrcData</code> and <code>RxParquetData</code>). Only the “standard” joins (<code>left_join</code>, <code>right_join</code>, <code>inner_join</code> and <code>full_join</code>) are supported.</p>
<p>As a final note, the Xdf data sources created by dplyrXdf will always contain absolute HDFS paths (that is, of the form <code>&quot;/path/to/file&quot;</code> rather than <code>&quot;file&quot;</code>. This is because of limitations in RevoScaleR support for relative paths in HDFS. The data source <em>inputs</em> to dplyrXdf verbs can contain relative paths; these will be internally translated to absolute.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
